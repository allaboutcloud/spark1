import boto3
from pyspark.sql import *
from pyspark.sql.types import *
#from delta import *
#from delta.tables import *
from pyspark.sql.functions import *

if __name__ == "__main__":




#    parser = argparse.ArgumentParser()
#    parser.add_argument('entity', help='entity name', type=int)
    #parser.add_argument('year', help='execution year', type=int)
    #parser.add_argument('month', help='execution month', type=int)
    #parser.add_argument('day', help='execution day', type=int)
    #parser.add_argument('opco', help='opco to be executed', type=str)
    #args = parser.parse_args()
    #assert args.opco in ('ES', 'UK', 'CSA'), 'Invalid OpCo code. Please input CSA, ES or UK to proceed.'
    ### read lookup file for enrity
    print("working on checking prefix in s3")

    client = boto3.client('s3')

    spark = SparkSession \
        .builder \
        .appName("DeltaLake") \
        .config("spark.jars", "/usr/lib/spark/jars/delta-core_2.11-0.6.1")\
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()


    spark.sparkContext.addPyFile("/usr/lib/spark/jars/delta-core_2.11-0.6.1.jar")
    spark.sparkContext.setLogLevel("ERROR")

    from delta import *
    from delta.tables import *

    entity_name="1sf_account"
    account="s3://vf-bdc-vb-euce1-dev-data/datalake/"+entity_name+"/year=2019/month=6/day=4"
    entity_source_location="s3://vf-bdc-vb-euce1-dev-data/datalake/"+entity_name+"/"
    entity_target_location="s3://get-leap-dev-emr-temp/deltalake"+entity_name
    #df=spark.read.format("csv").load(f"s3://vf-bdc-vb-euce1-dev-submission/{entity}",header=True, inferSchema=True)
    df=spark.read.format("parquet").load(entity_source_location)
    #df=spark.read.format("parquet").load(account)
    #df=spark.read.format("parquet").load(f"s3://vf-bdc-vb-euce1-dev-data/datalake/1sf_account/year=2019/month=6/day=4")
    #df.printSchema()

########################## WRITING for firsttime ###################
    KEYCOUNT=""
    response = client.list_objects_v2(Bucket='get-leap-dev-emr-temp',Prefix='deltalake/'+entity_name)
    for k,v in response.items():
        if( k=="KeyCount" ):
            print('++++++++++++++++++++'+k+'++++++++++'+str(v)+'++++++++++++++++++++++++++++++++')
            KEYCOUNT=v

    if ( v>0 ):
        df.write.format("delta").mode("overwrite").save(entity_target_location)
    #spark.read.format("delta").load("s3://get-leap-dev-emr-temp/delta-table-PoC/")
    #schema = StructType([StructField("Id", IntegerType(), True),StructField("name", StringType(), True),StructField("location", StringType(), True)])
    #delta_df=spark.read.format("delta").option("schema",schema).load("s3://get-leap-dev-emr-temp/delta-table-PoC/")
    #delta_df.columns=[("code","Name","location")]
    #delta_df.show()
    #print("The dataframe type {}".format(delta_df.dtypes))
    #print("The schema is {}".format(delta_df.schema))
    #spark.read.format("delta").load("s3://get-leap-dev-emr-temp/delta-table-PoC/").show()
    ######TEST DATA ##################
    ##################################
    #deltaTable = DeltaTable.forPath(spark, "s3://get-leap-dev-emr-temp/delta-table-PoC/")
#    df.write.format("delta").mode("overwrite").saveAsTable("OPPOTUNITY")
#    spark.sql("SELECT * FROM OPPOTUNITY")

    print("Testing  delta lake")

############### PRODUCT TABLE##########################
    print("product start")
    product_df=spark.read.format("csv").option("header","true").load("s3://vf-bdc-vb-euce1-dev-submission/delta/products.csv")
    product_df.printSchema()
    #product_df.show()
    product_df.write.format("delta").mode("overwrite").save("s3://get-leap-dev-emr-temp/product_delta/product")
    product_delta= DeltaTable.forPath(spark, "s3://get-leap-dev-emr-temp/product_delta/product")
    #fullHistoryDF = deltaTable.history()
    #fullHistoryDF.show()
    product_df1=spark.read.format("csv").option("header","true").load("s3://vf-bdc-vb-euce1-dev-submission/delta/product_ver1.csv")
    product_df1.printSchema()

    product_delta.alias("baseload").merge(
    product_df1.alias("newdata"),
    "baseload.product=newdata.product"
    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()

    spark.read.format("delta").load("s3://get-leap-dev-emr-temp/product_delta/product").show()